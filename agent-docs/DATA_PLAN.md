# Data Pipeline Plan: NASA Exoplanet Archive to Visualization

## Overview

This document outlines the process for transforming raw NASA Exoplanet Archive data into clean, visualization-ready JSON files. The goal is to maintain a transparent, reproducible pipeline from source data to final visualization.

## Problem Statement

The current visualization uses **synthetic data** generated by `src/utils/dataLoader.ts:generateSampleExoplanets()`. This produces statistically plausible but imagined planets. We need to replace this with real observational data from the NASA Exoplanet Archive.

### Source Data Analysis

**File**: `NASA-Exoplanet-Archive_PS_2025.12.26_07.44.02.csv`
- **Total rows**: 39,505 (parameter sets)
- **Unique planets**: 6,065 (confirmed exoplanets)
- **Default parameter sets**: 6,065 rows with `default_flag=1`

The discrepancy exists because the archive stores **multiple parameter sets per planet** from different publications/measurements. The `default_flag=1` rows contain the currently recommended values.

---

## Directory Structure

```
data-pipeline/
├── README.md                      # Pipeline documentation
├── 01-raw/                        # Untouched source data
│   └── NASA-Exoplanet-Archive_PS_2025.12.26_07.44.02.csv
├── 02-processed/                  # Intermediate processing steps
│   ├── step1-extract-defaults.csv
│   ├── step2-select-columns.csv
│   └── step3-clean-values.csv
├── 03-enriched/                   # Data with narrative additions
│   ├── planets-observed.json      # Pure observational data
│   └── planets-with-narrative.json # Includes editorial content
├── 04-final/                      # Ready for visualization
│   └── exoplanets.json
├── scripts/                       # Processing scripts
│   ├── 01-extract-defaults.py
│   ├── 02-select-and-clean.py
│   ├── 03-enrich-narrative.py
│   └── 04-generate-final.py
└── narrative/                     # Editorial content (clearly separated)
    ├── notable-planets.json       # Famous/interesting planets
    ├── planet-descriptions.json   # Human-readable descriptions
    └── SOURCES.md                 # Citations for narrative content
```

---

## Phase 1: Setup

### 1.1 Create Working Directory
```bash
mkdir -p data-pipeline/{01-raw,02-processed,03-enriched,04-final,scripts,narrative}
```

### 1.2 Copy Raw Data
```bash
cp "NASA-Exoplanet-Archive_PS_2025.12.26_07.44.02.csv" data-pipeline/01-raw/
```

### 1.3 Document Source
Create `data-pipeline/01-raw/SOURCE.md`:
- Download URL
- Download date/time
- Archive query parameters used
- Data version/release notes

---

## Phase 2: Extract Default Parameter Sets

### 2.1 Goal
Filter to only `default_flag=1` rows (one canonical measurement per planet).

### 2.2 Implementation
Script: `scripts/01-extract-defaults.py`

```python
# Filter criteria:
# - default_flag == 1
# - pl_controv_flag == 0 (exclude controversial detections)
```

### 2.3 Validation
- Output count should be ~6,065 planets
- Each `pl_name` should appear exactly once
- Log any duplicates or anomalies

### 2.4 Output
`02-processed/step1-extract-defaults.csv`

---

## Phase 3: Select Relevant Columns

### 3.1 Columns for Visualization

**Required (core visualization)**:
| Archive Column | Output Field | Description |
|---------------|--------------|-------------|
| `pl_name` | `name` | Planet designation |
| `hostname` | `hostStar` | Host star name |
| `pl_orbper` | `period` | Orbital period (days) |
| `pl_orbsmax` | `separation` | Semi-major axis (AU) |
| `pl_rade` | `radius` | Planet radius (Earth radii) |
| `pl_bmasse` | `mass` | Planet mass or M*sin(i) (Earth masses) |
| `pl_bmassprov` | `massProvenance` | Mass measurement type |
| `discoverymethod` | `detectionMethod` | Primary discovery method |
| `disc_year` | `discoveryYear` | Year of discovery |
| `disc_facility` | `facility` | Discovery facility |

**Supplementary (detail panel)**:
| Archive Column | Output Field | Description |
|---------------|--------------|-------------|
| `pl_eqt` | `temperature` | Equilibrium temperature (K) |
| `pl_dens` | `density` | Planet density (g/cm³) |
| `pl_orbeccen` | `eccentricity` | Orbital eccentricity |
| `pl_insol` | `insolation` | Insolation flux (Earth flux) |
| `st_spectype` | `starSpectralType` | Host star spectral type |
| `st_teff` | `starTemperature` | Host star temperature (K) |
| `st_rad` | `starRadius` | Host star radius (Solar radii) |
| `st_mass` | `starMass` | Host star mass (Solar masses) |
| `sy_dist` | `distance` | Distance from Earth (pc) |
| `ra`, `dec` | `coordinates` | Sky position |

### 3.2 Detection Method Mapping

Map archive `discoverymethod` values to visualization categories:

| Archive Value | Visualization ID | Notes |
|--------------|------------------|-------|
| `Radial Velocity` | `radial-velocity` | |
| `Transit` | `transit-kepler` or `transit-other` | Split by facility |
| `Microlensing` | `microlensing` | |
| `Imaging` | `direct-imaging` | |
| `Astrometry` | `astrometry` | |
| `Eclipse Timing Variations` | `other` | Rare method |
| `Pulsar Timing` | `other` | Rare method |
| `Disk Kinematics` | `other` | Rare method |

**Transit subdivision logic**:
- If `disc_facility` contains "Kepler" or "K2": `transit-kepler`
- Otherwise: `transit-other` (includes TESS, ground-based)

### 3.3 Output
`02-processed/step2-select-columns.csv`

---

## Phase 4: Clean and Validate Data

### 4.1 Handle Missing Values

**Strategy by field**:
| Field | If Missing |
|-------|-----------|
| `period` | Required - exclude planet if missing |
| `mass` | Use `pl_msinie` if available, else null |
| `radius` | Allow null (many RV planets lack radius) |
| `separation` | Calculate from period if missing (Kepler's 3rd law) |
| `temperature` | Allow null |
| `distance` | Allow null |

### 4.2 Derive Calculated Fields

```javascript
// Planet type classification based on mass/radius
planetType = classifyPlanet(mass, radius, period)

// Separation from period if missing (assumes solar-mass star)
if (!separation && period) {
  separation = Math.pow((period / 365.25) ** 2, 1/3)  // AU
}

// Generate stable ID
id = `exo-${pl_name.toLowerCase().replace(/\s+/g, '-')}`
```

### 4.3 Data Quality Checks

- Flag planets with implausible values (negative mass, period < 0.1 days, etc.)
- Log coordinate outliers
- Verify detection method coverage matches expected distribution

### 4.4 Output
`02-processed/step3-clean-values.csv`

---

## Phase 5: Enrich with Narrative Content

### 5.1 Observational Data (Untouched)

`03-enriched/planets-observed.json`:
- Pure data from archive
- No editorial additions
- Clearly marked as "OBSERVED_DATA"

### 5.2 Narrative Content (Clearly Marked)

`narrative/notable-planets.json`:
```json
{
  "_meta": {
    "type": "NARRATIVE_CONTENT",
    "source": "Editorial additions based on published literature",
    "lastUpdated": "2025-12-26"
  },
  "planets": {
    "proxima-cen-b": {
      "isNotable": true,
      "notableReason": "Closest known exoplanet to Earth",
      "description": "Rocky planet in the habitable zone of our nearest stellar neighbor",
      "sources": ["Anglada-Escude et al. 2016"]
    },
    "trappist-1-d": {
      "isNotable": true,
      "notableReason": "Part of the TRAPPIST-1 system with 7 Earth-sized planets",
      "description": "One of seven rocky planets in a compact system",
      "sources": ["Gillon et al. 2017"]
    }
  }
}
```

### 5.3 Combined Output

`03-enriched/planets-with-narrative.json`:
- Merge observed data with narrative
- Each planet has `_source` field: `"observed"` or `"narrative"`
- Narrative fields prefixed with `narrative_`

---

## Phase 6: Generate Final Output

### 6.1 Output Format

`04-final/exoplanets.json`:
```json
{
  "metadata": {
    "source": "NASA Exoplanet Archive",
    "sourceUrl": "https://exoplanetarchive.ipac.caltech.edu/",
    "downloadDate": "2025-12-26",
    "planetCount": 6065,
    "processedDate": "2025-12-26",
    "pipelineVersion": "1.0.0"
  },
  "planets": [
    {
      "id": "exo-kepler-442-b",
      "name": "Kepler-442 b",
      "hostStar": "Kepler-442",
      "period": 112.3053,
      "separation": 0.409,
      "mass": 2.34,
      "massProvenance": "Mass",
      "radius": 1.34,
      "detectionMethod": "transit-kepler",
      "discoveryYear": 2015,
      "facility": "Kepler",
      "temperature": 233,
      "distance": 342.1,
      "planetType": "super-earth",
      "isSolarSystem": false,
      "_observed": {
        "period": true,
        "mass": true,
        "radius": true
      },
      "_narrative": {
        "isNotable": true,
        "description": "One of the most Earth-like exoplanets known"
      }
    }
  ]
}
```

### 6.2 Copy to Visualization

```bash
cp data-pipeline/04-final/exoplanets.json public/data/exoplanets.json
```

### 6.3 Update dataLoader.ts

Replace `generateSampleExoplanets()` with:
```typescript
export async function loadExoplanets(): Promise<Planet[]> {
  const response = await fetch('/data/exoplanets.json')
  const data = await response.json()
  return data.planets
}
```

---

## Phase 7: Validation & Documentation

### 7.1 Validation Checks

- [ ] Planet count matches archive (6,065)
- [ ] No duplicate IDs
- [ ] All required fields present
- [ ] Detection method distribution matches expected
- [ ] Mass/radius values in plausible ranges
- [ ] Solar System planets NOT included (separate file)

### 7.2 Generate Statistics Report

`data-pipeline/STATS.md`:
- Total planets by detection method
- Missing value counts per field
- Planet type distribution
- Discovery year histogram
- Notable planets flagged

### 7.3 Documentation

Update `data-pipeline/README.md` with:
- Full pipeline instructions
- How to re-run with updated archive data
- Column mapping documentation
- Known limitations

---

## Execution Checklist

- [ ] Phase 1: Create directory structure
- [ ] Phase 1: Copy and document raw data
- [ ] Phase 2: Write and run extraction script
- [ ] Phase 2: Validate output count
- [ ] Phase 3: Write column selection script
- [ ] Phase 3: Implement detection method mapping
- [ ] Phase 4: Write cleaning script
- [ ] Phase 4: Handle missing values
- [ ] Phase 4: Run quality checks
- [ ] Phase 5: Create narrative content file
- [ ] Phase 5: Merge observed + narrative
- [ ] Phase 6: Generate final JSON
- [ ] Phase 6: Copy to public/data/
- [ ] Phase 6: Update dataLoader.ts
- [ ] Phase 7: Run validation checks
- [ ] Phase 7: Generate stats report
- [ ] Phase 7: Update documentation
- [ ] Final: Test in visualization
- [ ] Final: Commit changes

---

## Notes

### Why Python for Processing?

Python (with pandas) is used for data processing because:
1. pandas excels at CSV manipulation
2. Easy to inspect data interactively
3. Common in data science workflows
4. Scripts are readable and auditable

The final output is JSON for the TypeScript/React visualization.

### Handling Updates

When new archive data is released:
1. Download new CSV to `01-raw/`
2. Re-run pipeline scripts
3. Review diff of final output
4. Update metadata dates
5. Commit with clear changelog

### Separation of Concerns

- **Observed data**: Never modified, only filtered/reformatted
- **Narrative content**: Clearly marked, sourced, editable
- **Derived fields**: Calculated values documented with formulas
